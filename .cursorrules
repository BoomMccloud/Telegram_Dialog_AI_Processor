# Instructions

You are a multi-agent system coordinator, playing two roles in this environment: Planner and Executor. You will decide the next steps based on the current state of `Multi-Agent Scratchpad` section in the `.cursorrules` file. Your goal is to complete the user's (or business's) final requirements. The specific instructions are as follows:

## Role Descriptions

1. Planner

    * Responsibilities: Perform high-level analysis, break down tasks, define success criteria, evaluate current progress. When doing planning, always use high-intelligence models (OpenAI o1 via `tools/plan_exec_llm.py`). Don't rely on your own capabilities to do the planning.
    * Actions: Invoke the Planner by calling `.venv/bin/python tools/plan_exec_llm.py --prompt {any prompt}`. You can also include content from a specific file in the analysis by using the `--file` option: `.venv/bin/python tools/plan_exec_llm.py --prompt {any prompt} --file {path/to/file}`. It will print out a plan on how to revise the `.cursorrules` file. You then need to actually do the changes to the file. And then reread the file to see what's the next step.

2) Executor

    * Responsibilities: Execute specific tasks instructed by the Planner, such as writing code, running tests, handling implementation details, etc.. The key is you need to report progress or raise questions to the Planner at the right time, e.g. after completion some milestone or after you've hit a blocker.
    * Actions: When you complete a subtask or need assistance/more information, also make incremental writes or modifications to the `Multi-Agent Scratchpad` section in the `.cursorrules` file; update the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections. And then change to the Planner role.

## Document Conventions

* The `Multi-Agent Scratchpad` section in the `.cursorrules` file is divided into several sections as per the above structure. Please do not arbitrarily change the titles to avoid affecting subsequent reading.
* Sections like "Background and Motivation" and "Key Challenges and Analysis" are generally established by the Planner initially and gradually appended during task progress.
* "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" are mainly filled by the Executor, with the Planner reviewing and supplementing as needed.
* "Next Steps and Action Items" mainly contains specific execution steps written by the Planner for the Executor.

## Workflow Guidelines

* After you receive an initial prompt for a new task, update the "Background and Motivation" section, and then invoke the Planner to do the planning.
* When thinking as a Planner, always use the local command line `python tools/plan_exec_llm.py --prompt {any prompt}` to call the o1 model for deep analysis, recording results in sections like "Key Challenges and Analysis" or "High-level Task Breakdown". Also update the "Background and Motivation" section.
* When you as an Executor receive new instructions, use the existing cursor tools and workflow to execute those tasks. After completion, write back to the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections in the `Multi-Agent Scratchpad`.
* If unclear whether Planner or Executor is speaking, declare your current role in the output prompt.
* Continue the cycle unless the Planner explicitly indicates the entire project is complete or stopped. Communication between Planner and Executor is conducted through writing to or modifying the `Multi-Agent Scratchpad` section.

Please note:

* Note the task completion should only be announced by the Planner, not the Executor. If the Executor thinks the task is done, it should ask the Planner for confirmation. Then the Planner needs to do some cross-checking.
* Avoid rewriting the entire document unless necessary;
* Avoid deleting records left by other roles; you can append new paragraphs or mark old paragraphs as outdated;
* When new external information is needed, you can use command line tools (like search_engine.py, llm_api.py), but document the purpose and results of such requests;
* Before executing any large-scale changes or critical functionality, the Executor should first notify the Planner in "Executor's Feedback or Assistance Requests" to ensure everyone understands the consequences.
* During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

# Tools

Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

## Screenshot Verification
The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

1. Screenshot Capture:
```bash
.venv/bin/python tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

2. LLM Verification with Images:
```bash
.venv/bin/python tools/llm_api.py --prompt "Your verification question" --provider {openai|anthropic} --image path/to/screenshot.png
```

Example workflow:
```python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot
screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM
response = query_llm(
    "What is the background color and title of this webpage?",
    provider="openai",  # or "anthropic"
    image_path=screenshot_path
)
print(response)
```

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
.venv/bin/python ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
```

The LLM API supports multiple providers:
- OpenAI (default, model: gpt-4o)
- Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
- DeepSeek (model: deepseek-chat)
- Anthropic (model: claude-3-sonnet-20240229)
- Gemini (model: gemini-pro)
- Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```
.venv/bin/python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```
.venv/bin/python ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- You have a uv python venv in ./.venv. Always use it when running python scripts. It's a uv venv, so use `uv pip install` to install packages. And you need to activate it first. When you see errors like `no such file or directory: .venv/bin/uv`, that means you didn't activate the venv.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.

## Cursor learned

- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use 'seaborn-v0_8' instead of 'seaborn' as the style name due to recent seaborn version changes
- Use `gpt-4o` as the model name for OpenAI. It is the latest GPT model and has vision capabilities as well. `o1` is the most advanced and expensive model from OpenAI. Use it when you need to do reasoning, planning, or get blocked.
- Use `claude-3-5-sonnet-20241022` as the model name for Claude. It is the latest Claude model and has vision capabilities as well.
- When running Python scripts that import from other local modules, use `PYTHONPATH=.` to ensure Python can find the modules. For example: `PYTHONPATH=. python tools/plan_exec_llm.py` instead of just `python tools/plan_exec_llm.py`. This is especially important when using relative imports.

# Multi-Agent Scratchpad

## Background and Motivation

This is a Telegram dialog processing system consisting of two main components:

1. `tg_data_downloader.py`: A Telegram data collection service that:
   - Downloads messages from both group and private chats
   - Focuses on unread messages and messages from the last 24 hours
   - Stores the last 20 messages for each relevant dialog
   - Outputs data to telegram_data.json

2. `reply_only_llama3.2.py`: A dialog processing system that:
   - Uses the Llama model for generating responses
   - Implements a sophisticated message processing pipeline
   - Has built-in safety and validation mechanisms
   - Generates context-aware responses

The system appears to be designed for automated processing and responding to Telegram messages using LLM technology.

## Key Challenges and Analysis

1. Technical Architecture Challenges:
   - Integration of FastAPI backend with existing Python scripts
   - Implementation of secure QR code-based Telegram authentication
   - Management of concurrent user sessions and message processing
   - Handling of periodic background tasks for message downloading
   - Database design for storing user sessions and message history

2. Security Considerations:
   - Secure storage of Telegram API credentials:
     - Use environment variables for sensitive data
     - Implement credential rotation mechanism
     - Monitor for unauthorized access attempts
   - Implement comprehensive access control policies:
     - Rate limiting on API endpoints
     - IP-based blocking for suspicious activity
     - Session timeout and renewal policies
   - Regular security audits and vulnerability assessments
   - Implement HTTPS everywhere with certificate automation

3. Performance Risks:
   - Scalability of message processing with multiple users
   - Resource management for LLM model instances
   - Network latency in message processing pipeline
   - Database performance with large message volumes

## Verifiable Success Criteria

1. Authentication & Security:
   - Successful QR code-based Telegram login
   - Secure session management
   - Protected API endpoints
   - Encrypted data storage

2. Functionality:
   - Real-time message synchronization
   - Accurate message processing with LLama model
   - Two-way communication working in both web UI and Telegram
   - Background task execution for periodic updates

3. Performance:
   - Message processing latency under 5 seconds
   - UI responsiveness under 200ms
   - Successful handling of concurrent users
   - Efficient database queries

## High-level Task Breakdown

1. Backend Development (FastAPI):
   a. Project Setup:
      - Create a modular FastAPI project structure supporting scalability
      - Set up dependency management with a focus on security for API keys
      - Configure environment variables for both local and production environments
      - Implement database models with a focus on user sessions and message history

   b. Authentication System:
      - Develop a secure QR code-based authentication mechanism:
        - Generate unique QR codes with expiration timestamps
        - Implement Telegram bot for QR code validation
        - Handle concurrent login attempts securely
      - Define the QR code generation and scanning flow:
        1. User requests login -> Generate unique QR code
        2. User scans QR with Telegram -> Bot validates user
        3. WebSocket notification of successful auth
        4. Session creation and redirect to dashboard
      - Implement session management with JWT tokens
      - Add robust authentication middleware for API protection

   c. Core API Endpoints:
      - Message download endpoint
      - Message processing endpoint
      - User preferences endpoint
      - Status and health check endpoints

   d. Background Tasks:
      - Periodic message fetching service
      - Message processing queue
      - Notification system
      - Cleanup tasks

2. Frontend Development (Next.js on Vercel):
   a. UI Components:
      - Login page with QR code display (implemented using Next.js components)
      - Message dashboard with real-time updates (using Next.js App Router)
      - Chat interface with WebSocket integration
      - Settings panel with dynamic updates

   b. State Management:
      - User session handling through Next.js context or Redux
      - Real-time updates using WebSockets or Server-Sent Events (SSE) integrated with Next.js App Router
      - Error handling with Next.js error boundaries
      - Loading states using Next.js suspense features

   c. API Integration:
      - Next.js API routes for backend communication
      - Environment configuration for different deployments
      - WebSocket connection management
      - Error handling and retry logic

3. Integration Layer:
   a. Next.js and FastAPI Integration:
      - Use Next.js API routes to proxy requests to the FastAPI backend
      - Ensure CORS policies are configured to allow requests from Vercel
      - Adapt reply_only_llama3.2.py for web service compatibility
      - Maintain shared data models across frontend and backend for consistency
      - Implement robust error handling in both frontend and backend

   b. Database Integration:
      - Design schema for users and messages:
        ```sql
        -- Message History Table
        CREATE TABLE message_history (
            message_id UUID PRIMARY KEY,
            dialog_name VARCHAR(255),
            message_date TIMESTAMP,
            sender_name VARCHAR(255),
            message_text TEXT,
            INDEX(message_date)
        );

        -- Processing Results Table
        CREATE TABLE processing_results (
            result_id UUID PRIMARY KEY,
            message_id UUID NOT NULL,
            processed_text TEXT,
            response_text TEXT,
            processing_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (message_id) REFERENCES message_history(message_id)
        );

        -- Authentication Data Table
        CREATE TABLE authentication_data (
            auth_id UUID PRIMARY KEY,
            telegram_id BIGINT UNIQUE NOT NULL,
            encrypted_data BYTEA,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        ```
      - Implement direct database interactions using asyncpg
      - Create database migration scripts
      - Set up per-user database isolation

      Storage Strategy:
      1. Security:
         - One container per user for complete data isolation
         - Encrypt sensitive data (auth tokens, passwords)
         - Regular security audits and logging
      2. Performance:
         - Index critical fields (message_date)
         - Utilize PostgreSQL's built-in query cache
         - Implement efficient async database operations
      3. Deployment:
         - Docker Compose setup for easy deployment
         - Automated container creation per user
         - Simple backup and restore procedures

4. Testing & Deployment:
   a. Testing:
      - Unit tests for API endpoints
      - Integration tests for message processing
      - Frontend component tests using Jest and React Testing Library
      - End-to-end testing with Cypress
      - Load testing for concurrent user scenarios

   b. Deployment:
      Frontend (Vercel):
      ```yaml
      # vercel.json
      {
        "env": {
          "NEXT_PUBLIC_API_URL": "https://api.yourdomain.com",
          "NEXT_PUBLIC_WS_URL": "wss://api.yourdomain.com"
        },
        "build": {
          "env": {
            "NEXT_PUBLIC_API_URL": "https://api.yourdomain.com",
            "NEXT_PUBLIC_WS_URL": "wss://api.yourdomain.com"
          }
        }
      }
      ```

      Backend (Docker Compose):
      ```yaml
      version: '3.8'
      services:
        app:
          build: .
          environment:
            - POSTGRES_DB=${DB_NAME}
            - POSTGRES_USER=${DB_USER}
            - POSTGRES_PASSWORD=${DB_PASSWORD}
            - TELEGRAM_API_ID=${TELEGRAM_API_ID}
            - TELEGRAM_API_HASH=${TELEGRAM_API_HASH}
            - CORS_ORIGINS=${CORS_ORIGINS}
            - JWT_SECRET=${JWT_SECRET}
            - REDIS_URL=${REDIS_URL}
          volumes:
            - ./app:/app
          depends_on:
            - db
          ports:
            - "8000:8000"
          healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
            interval: 30s
            timeout: 10s
            retries: 3

        db:
          image: postgres:15
          environment:
            - POSTGRES_DB=${DB_NAME}
            - POSTGRES_USER=${DB_USER}
            - POSTGRES_PASSWORD=${DB_PASSWORD}
          volumes:
            - postgres_data:/var/lib/postgresql/data
          healthcheck:
            test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
            interval: 10s
            timeout: 5s
            retries: 5

      volumes:
        postgres_data:
      ```

   c. Monitoring and Observability:
      - System health monitoring:
        - CPU, memory, and disk usage
        - API endpoint response times
        - Database query performance
        - WebSocket connection status
      - Error tracking and alerting:
        - Centralized error logging
        - Alert thresholds for critical metrics
        - Automated incident response
      - Performance monitoring:
        - Real-time user metrics
        - Message processing latency
        - Database query analysis

   d. Backup and Recovery:
      - Automated daily database backups
      - Point-in-time recovery capability
      - Backup verification and testing
      - Disaster recovery procedures

## Development Workflow

1. Local Development:
   - Prerequisites:
     - Docker and Docker Compose
     - Node.js 18+ and npm/yarn
     - Python 3.9+
   - Setup steps:
     ```bash
     # Clone repository
     git clone <repository-url>
     cd <repository>

     # Frontend setup
     cd frontend
     npm install
     npm run dev

     # Backend setup
     cd ../backend
     python -m venv .venv
     source .venv/bin/activate
     pip install -r requirements.txt
     
     # Start services
     docker-compose up -d
     ```

2. Git Workflow:
   - Branch naming convention:
     - feature/* for new features
     - fix/* for bug fixes
     - chore/* for maintenance
   - Pull request requirements:
     - Code review by at least one team member
     - All tests passing
     - No security vulnerabilities
     - Documentation updated

3. CI/CD Pipeline:
   - On pull request:
     - Run linting and tests
     - Build Docker images
     - Deploy to staging environment
   - On merge to main:
     - Deploy to production
     - Run smoke tests
     - Monitor for errors

## Current Status / Progress Tracking

Status: Ready for Implementation
- Architecture updated for Next.js on Vercel frontend
- Backend configuration prepared for CORS and WebSocket support
- Deployment configurations defined for both frontend and backend

## MVP Development Focus

1. Core Features for MVP:
   - Implement QR code-based Telegram login in FastAPI
   - Set up message retrieval from Telegram using the bot
   - Process messages using the Llama model
   - Display messages and responses on a basic Next.js frontend

2. Next Steps and Action Items for MVP:
   - Set up FastAPI backend:
     - Initialize FastAPI application
     - Implement Telegram authentication via QR code
     - Create an endpoint for message processing
     - Ensure CORS is configured for Next.js frontend

   - Set up Next.js frontend:
     - Create a simple page to display QR code for login
     - Show incoming messages and generated responses
     - Configure environment variables for API communication

   - Deployment and Testing:
     - Deploy backend and frontend using Docker and Vercel
     - Test the end-to-end user journey from login to message display
     - Verify the integration between components

## Incremental Development Plan

### Sprint 1: Enhance Authentication and User Management
- Implement comprehensive access control policies
- Add user registration and profile management
- Improve session management with enhanced JWT token handling
- Conduct security audits and vulnerability assessments

### Sprint 2: Expand Message Processing Capabilities
- Integrate advanced message processing features within the Llama model
- Implement language support for additional languages
- Optimize message processing latency and resource management
- Introduce user-specific message processing preferences

### Sprint 3: Improve UI and User Experience
- Develop a rich message dashboard with advanced filtering and searching capabilities
- Enhance chat interface with multimedia support (images, videos, etc.)
- Implement real-time collaboration features for group chats
- Conduct user experience testing and feedback integration

### Sprint 4: Scale System Performance and Robustness
- Scale message processing for concurrent users
- Enhance database performance with optimized queries and indexing
- Conduct load testing and performance monitoring
- Implement backup and recovery strategies for data integrity

## Executor's Feedback or Assistance Requests

Ready to begin implementation with Next.js and FastAPI. Need to:
1. Set up Next.js project with App Router
2. Configure FastAPI for CORS and WebSocket support
3. Test local development environment