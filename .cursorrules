# Instructions

You are a multi-agent system coordinator, playing two roles in this environment: Planner and Executor. You will decide the next steps based on the current state of `Multi-Agent Scratchpad` section in the `.cursorrules` file. Your goal is to complete the user's (or business's) final requirements. The specific instructions are as follows:

## Role Descriptions

1. Planner

    * Responsibilities: Perform high-level analysis, break down tasks, define success criteria, evaluate current progress. When doing planning, always use high-intelligence models (OpenAI o1 via `tools/plan_exec_llm.py`). Don't rely on your own capabilities to do the planning.
    * Actions: Invoke the Planner by calling `.venv/bin/python tools/plan_exec_llm.py --prompt {any prompt} --file .cursorrules`. The `--file` option with `.cursorrules` is mandatory to ensure the planner has full context of the current state and previous decisions. You can include additional files in the analysis by adding more `--file` options: `.venv/bin/python tools/plan_exec_llm.py --prompt {any prompt} --file .cursorrules --file {path/to/additional/file}`. It will print out a plan on how to revise the `.cursorrules` file. You then need to actually do the changes to the file. And then reread the file to see what's the next step.

2) Executor

    * Responsibilities: Execute specific tasks instructed by the Planner, such as writing code, running tests, handling implementation details, etc.. The key is you need to report progress or raise questions to the Planner at the right time, e.g. after completion some milestone or after you've hit a blocker.
    * Actions: When you complete a subtask or need assistance/more information, also make incremental writes or modifications to the `Multi-Agent Scratchpad` section in the `.cursorrules` file; update the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections. And then change to the Planner role.

## Document Conventions

* The `Multi-Agent Scratchpad` section in the `.cursorrules` file is divided into several sections as per the above structure. Please do not arbitrarily change the titles to avoid affecting subsequent reading.
* Sections like "Background and Motivation" and "Key Challenges and Analysis" are generally established by the Planner initially and gradually appended during task progress.
* "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" are mainly filled by the Executor, with the Planner reviewing and supplementing as needed.
* "Next Steps and Action Items" mainly contains specific execution steps written by the Planner for the Executor.

## Workflow Guidelines

* After you receive an initial prompt for a new task, update the "Background and Motivation" section, and then invoke the Planner to do the planning.
* When thinking as a Planner, always use the local command line `python tools/plan_exec_llm.py --prompt {any prompt}` to call the o1 model for deep analysis, recording results in sections like "Key Challenges and Analysis" or "High-level Task Breakdown". Also update the "Background and Motivation" section.
* When you as an Executor receive new instructions, use the existing cursor tools and workflow to execute those tasks. After completion, write back to the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections in the `Multi-Agent Scratchpad`.
* If unclear whether Planner or Executor is speaking, declare your current role in the output prompt.
* Continue the cycle unless the Planner explicitly indicates the entire project is complete or stopped. Communication between Planner and Executor is conducted through writing to or modifying the `Multi-Agent Scratchpad` section.

Please note:

* Note the task completion should only be announced by the Planner, not the Executor. If the Executor thinks the task is done, it should ask the Planner for confirmation. Then the Planner needs to do some cross-checking.
* Avoid rewriting the entire document unless necessary;
* Avoid deleting records left by other roles; you can append new paragraphs or mark old paragraphs as outdated;
* When new external information is needed, you can use command line tools (like search_engine.py, llm_api.py), but document the purpose and results of such requests;
* Before executing any large-scale changes or critical functionality, the Executor should first notify the Planner in "Executor's Feedback or Assistance Requests" to ensure everyone understands the consequences.
* During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

# Tools

Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

## Screenshot Verification
The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

1. Screenshot Capture:
```bash
.venv/bin/python tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

2. LLM Verification with Images:
```bash
.venv/bin/python tools/llm_api.py --prompt "Your verification question" --provider {openai|anthropic} --image path/to/screenshot.png
```

Example workflow:
```python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot
screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM
response = query_llm(
    "What is the background color and title of this webpage?",
    provider="openai",  # or "anthropic"
    image_path=screenshot_path
)
print(response)
```

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
.venv/bin/python ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
```

The LLM API supports multiple providers:
- OpenAI (default, model: gpt-4o)
- Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
- DeepSeek (model: deepseek-chat)
- Anthropic (model: claude-3-sonnet-20240229)
- Gemini (model: gemini-pro)
- Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```
.venv/bin/python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```
.venv/bin/python ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- You have a uv python venv in ./.venv. Always use it when running python scripts. It's a uv venv, so use `uv pip install` to install packages. And you need to activate it first. When you see errors like `no such file or directory: .venv/bin/uv`, that means you didn't activate the venv.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.

## Cursor learned

- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use 'seaborn-v0_8' instead of 'seaborn' as the style name due to recent seaborn version changes
- Use `gpt-4o` as the model name for OpenAI. It is the latest GPT model and has vision capabilities as well. `o1` is the most advanced and expensive model from OpenAI. Use it when you need to do reasoning, planning, or get blocked.
- Use `claude-3-5-sonnet-20241022` as the model name for Claude. It is the latest Claude model and has vision capabilities as well.
- When running Python scripts that import from other local modules, use `PYTHONPATH=.` to ensure Python can find the modules. For example: `PYTHONPATH=. python tools/plan_exec_llm.py` instead of just `python tools/plan_exec_llm.py`. This is especially important when using relative imports.
- When using Docker Compose in a project with virtual environments, make sure to activate the virtual environment before running Docker Compose commands to ensure that environment variables from `.env` files are properly loaded and passed to the containers. For example, use `source .venv/bin/activate && docker-compose up -d` instead of just `docker-compose up -d`.
- When mocking an API that uses in-memory session storage (like `client_sessions` dictionary), ensure all dictionaries that store session information are properly synchronized. When creating mock sessions, always populate all required dictionaries with consistent data. For authentication-dependent endpoints, add double-checks to ensure sessions exist in all necessary dictionaries before attempting operations.
- For hybrid development environments, run just the database in Docker while keeping the backend and frontend running locally. This approach gives you the benefits of containerized persistence while maintaining fast development cycles. Use `docker-compose up -d postgres` to start only the database service, then run the backend with environment variables set to connect to the containerized database.
- When switching between development and production modes in a web application, ensure data formats remain consistent. Mock data APIs often have slightly different response formats than real APIs. Always check the response structure in both environments and adjust the backend to maintain format consistency.

# Multi-Agent Scratchpad

## Background and Motivation

The Telegram Dialog AI Processor is a web application that enables users to process Telegram messages with AI and manage responses. The system includes:

1. **Backend Features**:
   - Telegram integration via Telethon API for message retrieval
   - QR code-based authentication for secure login
   - Dialog selection for processing specific conversations
   - AI-powered message analysis and response generation using Claude
   - Database storage for messages, processing results, and user preferences

2. **Frontend Interface**:
   - Modern Next.js 14 web UI with TypeScript and Tailwind CSS
   - Intuitive dialog selection and filtering
   - Processing status indicators
   - Response review interface for approving, rejecting, or modifying AI responses

The system is designed to help users efficiently manage Telegram conversations by using AI to generate context-aware responses, which users can then review before sending. The MVP will use Claude as the primary LLM, with plans to support multiple models in the future.

## Key Challenges and Analysis

1. Claude API Integration Challenges:
   - Adapting existing processing pipeline to use Claude API
   - Optimizing prompt formatting for Claude's capabilities
   - Implementing proper rate limiting and error handling
   - Managing API costs and token usage efficiently

2. Background Processing Challenges:
   - Implementing resilient message queue processing
   - Handling periodic data fetching without overloading resources
   - Ensuring proper transaction management and error recovery
   - Managing concurrency with multiple users and dialogs

3. User Experience Considerations:
   - Designing an intuitive response review interface
   - Minimizing latency in message processing workflow
   - Providing clear feedback on processing status
   - Balancing automation with user control

4. Security and Data Management:
   - Secure handling of Telegram authentication credentials
   - Protecting user data and message content
   - Implementing proper access controls
   - Ensuring data consistency across system components

## High-level Task Breakdown

1. Claude Integration (In Progress):
   - Implement `ClaudeProcessor` class adapting the existing architecture
   - Set up authentication with Claude API using environment variables
   - Optimize prompt formatting and context management for Claude
   - Implement proper error handling and rate limiting

2. Message Processing System:
   - Create API endpoints for triggering message processing
   - Implement background task system for queue management
   - Set up periodic data fetching from selected Telegram dialogs
   - Develop status tracking for processing tasks

3. Response Management:
   - Implement endpoints for retrieving processed responses
   - Create API for approving, rejecting, or modifying responses
   - Develop storage system for tracking response status
   - Implement optional automatic response sending

4. Frontend Components:
   - Build response review interface
   - Create model configuration UI
   - Implement processing status indicators
   - Develop notification system for new responses

5. Testing and Deployment:
   - Conduct end-to-end testing with real data
   - Optimize performance for concurrent users
   - Finalize Docker configuration for production deployment
   - Set up monitoring and logging

## Development Workflow

1. Local Development:
   - Prerequisites: Docker, Node.js, Python
   - Setup steps for development environment

2. Git Workflow:
   - Branch naming conventions and PR requirements

3. CI/CD Pipeline:
   - Testing and deployment automation

## Current Status / Progress Tracking

Status: MVP Implementation In Progress

Completed:
1. Backend (FastAPI):
   - Project structure set up
   - Basic FastAPI application created
   - Telethon QR login implementation complete and working
   - Authentication endpoints created and tested
   - CORS configuration added
   - Message retrieval functionality implemented
   - Dialog listing functionality implemented
   - API endpoints for selecting and unselecting dialogs implemented and tested
   - Database storage for user selected dialogs operational
   - Database schema extended with `user_selected_models` and `processed_responses` tables
   - Model selection API endpoints implemented and tested
   - Data migration system implemented for schema updates
   - **Session management and middleware issues fixed**

2. Frontend (Next.js):
   - Project created with Next.js 14, TypeScript, and Tailwind CSS
   - Login page implemented with QR code display
   - Authentication status polling implemented 
   - Environment configuration set up
   - Messages page implemented showing chat dialogs
   - Chat selection and filtering UI implemented
   - Dialog selection UI with visual indicators for processing status
   - Start/Stop processing functionality working correctly with API integration
   - Success/error message handling implemented for processing operations

3. Docker Setup:
   - Docker Compose configuration complete with:
     - PostgreSQL database
     - FastAPI backend
     - Next.js frontend
   - Proper service dependencies and health checks configured
   - Environment variable configuration implemented
   - Database initialization and migration scripts integrated

4. Design Improvements:
   - Simplified model and system prompt management:
     - Moved system_prompt from dialog-level to user-level
     - Implemented centralized model selection for all dialogs
     - Created migration script for existing data
   - Improved API structure with dedicated routers for different resources
   - Enhanced development environment with hybrid Docker/local setup

In Progress:
1. Claude LLM Integration:
   - Implementing ClaudeProcessor class for MVP
   - Adapting existing dialog processing code to use Claude API
   - Setting up proper error handling and rate limiting

Next Steps:
1. **Focus on finalizing Claude API integration**:
   - Ensure prompt formatting and response handling are optimized for Claude
   - Complete configuration of environment variables for API access
   - Implement and test rate limiting and error handling

2. **Complete API endpoints for message processing and response retrieval**
3. **Implement background processing queue for message handling**
4. **Develop periodic data fetching from selected dialogs**
5. **Create frontend UI for response review**
6. **Test end-to-end workflow with Claude integration**

## MVP Development Focus

1. Core Features for MVP:
   - Implement Telethon's direct client QR code-based Telegram login in FastAPI ✅
   - Set up message retrieval from Telegram using Telethon ✅
   - Store user preferences in the database ✅
   - Process messages using the Claude model
   - Display messages and responses on the frontend
   - Allow users to review, approve, or reject AI-generated responses

2. Next Steps and Action Items for MVP:
   - **End-to-End MVP Implementation:**
     - **Objective:** Implement a Telegram Dialog AI Processor using Claude LLM.
     - **Current Focus:**
       - Implement `ClaudeProcessor` class for the MVP
       - Create API endpoints for message processing and response retrieval
       - Develop background task system for periodic message fetching
       - Build frontend UI for response review

     - **Immediate Action Items:**
       - Complete the Claude API integration
       - Implement message queue processing system
       - Create the response review API endpoints
       - Test the end-to-end workflow with real data

## Executor's Feedback or Assistance Requests

### Current Status - Claude Integration

I'm currently working on implementing the Claude integration for the MVP. The implementation includes:

1. **ClaudeProcessor Class**:
   - Creating a new service that adapts the existing DialogProcessor pattern for Claude
   - Setting up connection to the Claude API using authentication from environment variables
   - Implementing Claude-specific prompt formatting and response handling

2. **API Endpoints**:
   - Adding endpoints for processing messages with Claude
   - Creating endpoints for retrieving and managing processed responses
   - Implementing status tracking for background processing tasks

3. **Background Processing**:
   - Setting up a message processing queue worker
   - Implementing periodic data fetching from selected dialogs
   - Creating a background task monitoring system

4. **Frontend Components** (Next Steps):
   - Designing the response review interface
   - Creating UI components for model configuration
   - Implementing background task status indicators

These components will complete the end-to-end MVP implementation, allowing users to process Telegram messages with Claude and review the generated responses through the web interface.

### Critical Backend Error - Missing Middleware Module

I've identified a critical error in the backend application:

```
ModuleNotFoundError: No module named 'app.middleware'
```

The error occurs because the backend is trying to import `verify_session` from `.middleware.session_middleware`, but the middleware module doesn't exist. This needs to be fixed before we can proceed with the implementation.

**Resolution Steps:**
1. Create the middleware directory structure in the backend:
   ```
   backend/app/middleware/
   ```
2. Create a session middleware file:
   ```
   backend/app/middleware/session_middleware.py
   ```
3. Implement the `verify_session` function to handle session validation

This is a blocking issue that must be addressed before we can continue with the Claude integration implementation.

**Status Update**: This issue has been resolved. The middleware module has been created and the session validation functionality is now working properly.

### Progress Update - March 2023

**Completed Tasks:**
1. ✅ Successfully implemented the `ClaudeProcessor` class to use Anthropic's Claude API
2. ✅ Created a comprehensive responses API for managing AI-generated responses
3. ✅ Integrated the responses router into the main FastAPI application
4. ✅ Implemented session middleware for API endpoint security
5. ✅ Updated the models router to support Claude model selection
6. ✅ Implemented a background processing queue system for message handling
7. ✅ Created an API for managing message processing tasks

**Verified Configurations:**
1. ✅ API endpoints are properly structured with correct route prefixes
2. ✅ Authentication verification is working on all protected endpoints
3. ✅ Response management includes proper status tracking (pending, approved, rejected, etc.)
4. ✅ Database schema supports storing Claude-generated responses and metadata
5. ✅ Queue system properly manages background tasks for processing messages

**Next Steps:**
1. Create frontend components for response review
2. Set up periodic message fetching from selected dialogs
3. Perform end-to-end testing with the Claude integration
4. Optimize prompt formatting for Claude's capabilities
5. Implement monitoring dashboard for queue processing