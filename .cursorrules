# Instructions

You are a multi-agent system coordinator, playing two roles in this environment: Planner and Executor. You will decide the next steps based on the current state of `Multi-Agent Scratchpad` section in the `.cursorrules` file. Your goal is to complete the user's (or business's) final requirements. The specific instructions are as follows:

## Role Descriptions

1. Planner

    * Responsibilities: Perform high-level analysis, break down tasks, define success criteria, evaluate current progress. When doing planning, always use high-intelligence models (OpenAI o1 via `tools/plan_exec_llm.py`). Don't rely on your own capabilities to do the planning.
    * Actions: Invoke the Planner by calling `.venv/bin/python tools/plan_exec_llm.py --prompt {any prompt} --file .cursorrules`. The `--file` option with `.cursorrules` is mandatory to ensure the planner has full context of the current state and previous decisions. You can include additional files in the analysis by adding more `--file` options: `.venv/bin/python tools/plan_exec_llm.py --prompt {any prompt} --file .cursorrules --file {path/to/additional/file}`. It will print out a plan on how to revise the `.cursorrules` file. You then need to actually do the changes to the file. And then reread the file to see what's the next step.

2) Executor

    * Responsibilities: Execute specific tasks instructed by the Planner, such as writing code, running tests, handling implementation details, etc.. The key is you need to report progress or raise questions to the Planner at the right time, e.g. after completion some milestone or after you've hit a blocker.
    * Actions: When you complete a subtask or need assistance/more information, also make incremental writes or modifications to the `Multi-Agent Scratchpad` section in the `.cursorrules` file; update the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections. And then change to the Planner role.

## Document Conventions

* The `Multi-Agent Scratchpad` section in the `.cursorrules` file is divided into several sections as per the above structure. Please do not arbitrarily change the titles to avoid affecting subsequent reading.
* Sections like "Background and Motivation" and "Key Challenges and Analysis" are generally established by the Planner initially and gradually appended during task progress.
* "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" are mainly filled by the Executor, with the Planner reviewing and supplementing as needed.
* "Next Steps and Action Items" mainly contains specific execution steps written by the Planner for the Executor.

## Workflow Guidelines

* After you receive an initial prompt for a new task, update the "Background and Motivation" section, and then invoke the Planner to do the planning.
* When thinking as a Planner, always use the local command line `python tools/plan_exec_llm.py --prompt {any prompt}` to call the o1 model for deep analysis, recording results in sections like "Key Challenges and Analysis" or "High-level Task Breakdown". Also update the "Background and Motivation" section.
* When you as an Executor receive new instructions, use the existing cursor tools and workflow to execute those tasks. After completion, write back to the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections in the `Multi-Agent Scratchpad`.
* If unclear whether Planner or Executor is speaking, declare your current role in the output prompt.
* Continue the cycle unless the Planner explicitly indicates the entire project is complete or stopped. Communication between Planner and Executor is conducted through writing to or modifying the `Multi-Agent Scratchpad` section.

Please note:

* Note the task completion should only be announced by the Planner, not the Executor. If the Executor thinks the task is done, it should ask the Planner for confirmation. Then the Planner needs to do some cross-checking.
* Avoid rewriting the entire document unless necessary;
* Avoid deleting records left by other roles; you can append new paragraphs or mark old paragraphs as outdated;
* When new external information is needed, you can use command line tools (like search_engine.py, llm_api.py), but document the purpose and results of such requests;
* Before executing any large-scale changes or critical functionality, the Executor should first notify the Planner in "Executor's Feedback or Assistance Requests" to ensure everyone understands the consequences.
* During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

# Tools

Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

## Screenshot Verification
The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

1. Screenshot Capture:
```bash
.venv/bin/python tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

2. LLM Verification with Images:
```bash
.venv/bin/python tools/llm_api.py --prompt "Your verification question" --provider {openai|anthropic} --image path/to/screenshot.png
```

Example workflow:
```python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot
screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM
response = query_llm(
    "What is the background color and title of this webpage?",
    provider="openai",  # or "anthropic"
    image_path=screenshot_path
)
print(response)
```

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
.venv/bin/python ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
```

The LLM API supports multiple providers:
- OpenAI (default, model: gpt-4o)
- Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
- DeepSeek (model: deepseek-chat)
- Anthropic (model: claude-3-sonnet-20240229)
- Gemini (model: gemini-pro)
- Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```
.venv/bin/python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```
.venv/bin/python ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- You have a uv python venv in ./.venv. Always use it when running python scripts. It's a uv venv, so use `uv pip install` to install packages. And you need to activate it first. When you see errors like `no such file or directory: .venv/bin/uv`, that means you didn't activate the venv.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.

## Cursor learned

- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use 'seaborn-v0_8' instead of 'seaborn' as the style name due to recent seaborn version changes
- Use `gpt-4o` as the model name for OpenAI. It is the latest GPT model and has vision capabilities as well. `o1` is the most advanced and expensive model from OpenAI. Use it when you need to do reasoning, planning, or get blocked.
- Use `claude-3-5-sonnet-20241022` as the model name for Claude. It is the latest Claude model and has vision capabilities as well.
- When running Python scripts that import from other local modules, use `PYTHONPATH=.` to ensure Python can find the modules. For example: `PYTHONPATH=. python tools/plan_exec_llm.py` instead of just `python tools/plan_exec_llm.py`. This is especially important when using relative imports.

# Multi-Agent Scratchpad

## Background and Motivation

This is a Telegram dialog processing system consisting of two main components:

1. `tg_data_downloader.py`: A Telegram data collection service that:
   - Downloads messages from both group and private chats
   - Focuses on unread messages and messages from the last 24 hours
   - Stores the last 20 messages for each relevant dialog
   - Outputs data to telegram_data.json

2. `reply_only_llama3.2.py`: A dialog processing system that:
   - Uses the Llama model for generating responses
   - Implements a sophisticated message processing pipeline
   - Has built-in safety and validation mechanisms
   - Generates context-aware responses

The system appears to be designed for automated processing and responding to Telegram messages using LLM technology.

## Key Challenges and Analysis

1. Technical Architecture Challenges:
   - Integration of FastAPI backend with existing Python scripts
   - Implementation of Telethon's direct client authentication with QR login
   - Management of concurrent user sessions and message processing
   - Handling of periodic background tasks for message downloading
   - Database design for storing user sessions and message history

2. Security Considerations:
   - Secure storage of Telegram API credentials:
     - Use environment variables for sensitive data
     - Implement credential rotation mechanism
     - Monitor for unauthorized access attempts
   - Implement comprehensive access control policies:
     - Rate limiting on API endpoints
     - IP-based blocking for suspicious activity
     - Session timeout and renewal policies
   - Regular security audits and vulnerability assessments
   - Implement HTTPS everywhere with certificate automation

3. Performance Risks:
   - Scalability of message processing with multiple users
   - Resource management for LLM model instances
   - Network latency in message processing pipeline
   - Database performance with large message volumes

## Verifiable Success Criteria

1. Authentication & Security:
   - Successful QR code-based Telegram login
   - Secure session management
   - Protected API endpoints
   - Encrypted data storage

2. Functionality:
   - Real-time message synchronization
   - Accurate message processing with LLama model
   - Two-way communication working in both web UI and Telegram
   - Background task execution for periodic updates

3. Performance:
   - Message processing latency under 5 seconds
   - UI responsiveness under 200ms
   - Successful handling of concurrent users
   - Efficient database queries

## High-level Task Breakdown

1. Backend Development (FastAPI):
   a. Project Setup:
      - Create a modular FastAPI project structure supporting scalability
      - Set up dependency management with a focus on security for API keys
      - Configure environment variables for both local and production environments
      - Implement database models with a focus on user sessions and message history

   b. Database Implementation (PostgreSQL):
      - Schema Design:
        ```sql
        -- Message History Table
        CREATE TABLE message_history (
            message_id UUID PRIMARY KEY,
            dialog_id BIGINT NOT NULL,
            telegram_message_id BIGINT NOT NULL,
            dialog_name VARCHAR(255),
            message_date TIMESTAMP WITH TIME ZONE,
            sender_name VARCHAR(255),
            message_text TEXT,
            is_processed BOOLEAN DEFAULT false,
            embedding_vector vector(1536),
            UNIQUE(dialog_id, telegram_message_id),
            INDEX(message_date, is_processed)
        );

        -- Processing Results Table
        CREATE TABLE processing_results (
            result_id UUID PRIMARY KEY,
            message_id UUID NOT NULL,
            processed_text TEXT,
            response_text TEXT,
            context_messages JSONB,
            processing_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            auto_reply_sent BOOLEAN DEFAULT false,
            user_interaction_status VARCHAR(20) DEFAULT 'pending' CHECK (user_interaction_status IN ('pending', 'used', 'rejected', 'edited')),
            edited_response_text TEXT,
            interaction_date TIMESTAMP WITH TIME ZONE,
            user_feedback TEXT,
            FOREIGN KEY (message_id) REFERENCES message_history(message_id)
        );

        -- Authentication Data Table
        CREATE TABLE authentication_data (
            auth_id UUID PRIMARY KEY,
            telegram_id BIGINT UNIQUE NOT NULL,
            session_data JSONB,
            encrypted_credentials BYTEA,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            last_active_at TIMESTAMP WITH TIME ZONE,
            is_active BOOLEAN DEFAULT true
        );

        -- Processing Queue Table
        CREATE TABLE processing_queue (
            queue_id UUID PRIMARY KEY,
            message_id UUID NOT NULL,
            priority INTEGER DEFAULT 0,
            status VARCHAR(20) DEFAULT 'pending',
            created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            started_at TIMESTAMP WITH TIME ZONE,
            completed_at TIMESTAMP WITH TIME ZONE,
            error_message TEXT,
            FOREIGN KEY (message_id) REFERENCES message_history(message_id)
        );

        -- Priority Processing Index (NEW)
        CREATE INDEX idx_processing_priority ON processing_queue 
            USING BTREE (status, priority DESC, created_at);

        -- Auto-reply Rules Table
        CREATE TABLE auto_reply_rules (
            rule_id UUID PRIMARY KEY,
            dialog_id BIGINT NOT NULL,
            pattern TEXT,
            response_template TEXT,
            is_active BOOLEAN DEFAULT true,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            last_triggered_at TIMESTAMP WITH TIME ZONE
        );

        -- User Selected Dialogs Table
        CREATE TABLE user_selected_dialogs (
            selection_id UUID PRIMARY KEY,
            user_id BIGINT NOT NULL,
            dialog_id BIGINT NOT NULL,
            dialog_name VARCHAR(255),
            is_active BOOLEAN DEFAULT true,
            processing_enabled BOOLEAN DEFAULT true,
            auto_reply_enabled BOOLEAN DEFAULT false,
            response_approval_required BOOLEAN DEFAULT true,
            priority INTEGER DEFAULT 0,
            created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
            last_processed_at TIMESTAMP WITH TIME ZONE,
            processing_settings JSONB,
            UNIQUE(user_id, dialog_id)
        );
        ```

      - Migration Strategy:
        - Use Alembic for database migrations
        - Version control all schema changes
        - Implement rollback procedures
        - Test migrations in staging environment

      - Backup and Recovery:
        - Automated daily backups using pg_dump
        - WAL archiving for point-in-time recovery
        - Regular backup testing and validation
        - Documented recovery procedures

      - Performance Optimization:
        - Implement efficient indexing strategy
        - Use connection pooling with asyncpg
        - Monitor and optimize query performance
        - Implement caching where appropriate

      - FastAPI Integration:
        - Use Pydantic models for data validation
        - Implement dependency injection for DB sessions
        - Handle database errors gracefully
        - Implement database transaction management

   c. Authentication System:
      - Implement Telethon's direct client QR code-based authentication:
        - Use Telethon's native QR login flow for authentication
        - Ensure QR code generation and validation align with Telethon's requirements
        - Simplify the authentication process by removing Telegram bot validation
        - Update session management to integrate with Telethon's authentication
      - Implement session management with JWT tokens
      - Add robust authentication middleware for API protection

   d. Core API Endpoints:
      - Message download endpoint
      - Message processing endpoint
      - User preferences endpoint
      - Status and health check endpoints

   e. Background Tasks:
      - Periodic message fetching service
      - Message processing queue
      - Notification system
      - Cleanup tasks

2. Frontend Development (Next.js on Vercel):
   a. UI Components:
      - Login page with QR code display (implemented using Next.js components)
      - Message dashboard with real-time updates (using Next.js App Router)
      - Chat interface with WebSocket integration
      - Settings panel with dynamic updates

   b. State Management:
      - User session handling through Next.js context or Redux
      - Real-time updates using WebSockets or Server-Sent Events (SSE) integrated with Next.js App Router
      - Error handling with Next.js error boundaries
      - Loading states using Next.js suspense features

   c. API Integration:
      - Next.js API routes for backend communication
      - Environment configuration for different deployments
      - WebSocket connection management
      - Error handling and retry logic

3. Integration Layer:
   a. Next.js and FastAPI Integration:
      - Use Next.js API routes to proxy requests to the FastAPI backend
      - Ensure CORS policies are configured to allow requests from Vercel
      - Adapt reply_only_llama3.2.py for web service compatibility
      - Maintain shared data models across frontend and backend for consistency
      - Implement robust error handling in both frontend and backend

   b. Database Integration:
      - Design schema for users and messages:
        ```sql
        -- Message History Table
        CREATE TABLE message_history (
            message_id UUID PRIMARY KEY,
            dialog_name VARCHAR(255),
            message_date TIMESTAMP,
            sender_name VARCHAR(255),
            message_text TEXT,
            INDEX(message_date)
        );

        -- Processing Results Table
        CREATE TABLE processing_results (
            result_id UUID PRIMARY KEY,
            message_id UUID NOT NULL,
            processed_text TEXT,
            response_text TEXT,
            processing_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (message_id) REFERENCES message_history(message_id)
        );

        -- Authentication Data Table
        CREATE TABLE authentication_data (
            auth_id UUID PRIMARY KEY,
            telegram_id BIGINT UNIQUE NOT NULL,
            encrypted_data BYTEA,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        ```
      - Implement direct database interactions using asyncpg
      - Create database migration scripts
      - Set up per-user database isolation

      Storage Strategy:
      1. Security:
         - One container per user for complete data isolation
         - Encrypt sensitive data (auth tokens, passwords)
         - Regular security audits and logging
      2. Performance:
         - Optimize PostgreSQL configurations for single-user performance:
           ```ini
           # postgresql.conf optimizations for single-user instance
           max_connections = 20                    # Reduced for single user
           shared_buffers = '1GB'                 # 25% of container memory
           work_mem = '64MB'                      # Larger per-connection work mem
           maintenance_work_mem = '256MB'         # Larger for vacuum operations
           effective_cache_size = '3GB'           # 75% of container memory
           random_page_cost = 1.1                 # Assuming SSD storage
           effective_io_concurrency = 200         # Higher for SSD
           ```
         - Index critical fields for efficient querying:
           ```sql
           -- Optimized indexes for single-user performance
           CREATE INDEX idx_message_history_date ON message_history (message_date DESC);
           CREATE INDEX idx_message_history_dialog ON message_history (dialog_id, message_date DESC);
           CREATE INDEX idx_processing_queue_status ON processing_queue (status, priority DESC);
           CREATE INDEX idx_auth_data_active ON authentication_data (is_active, last_active_at DESC);
           CREATE INDEX idx_user_selected_dialogs_user ON user_selected_dialogs (user_id, is_active, priority DESC);
           ```
         - Implement efficient async database operations using connection pooling
      3. Deployment:
         - Use Docker to ensure each user has an isolated environment:
           ```yaml
           # docker-compose.yml for single user
           version: '3.8'
           services:
             app:
               build: .
               environment:
                 - POSTGRES_DB=user_${USER_ID}
                 - POSTGRES_USER=app
                 - POSTGRES_PASSWORD=${DB_PASSWORD}
                 - TELEGRAM_API_ID=${TELEGRAM_API_ID}
                 - TELEGRAM_API_HASH=${TELEGRAM_API_HASH}
                 - JWT_SECRET=${JWT_SECRET}
               volumes:
                 - ./app:/app
                 - ./data:/data
               depends_on:
                 - db
               ports:
                 - "${PORT}:8000"
               deploy:
                 resources:
                   limits:
                     cpus: '2.0'
                     memory: 4G
               healthcheck:
                 test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
                 interval: 30s
                 timeout: 10s
                 retries: 3

             db:
               image: postgres:15
               environment:
                 - POSTGRES_DB=user_${USER_ID}
                 - POSTGRES_USER=app
                 - POSTGRES_PASSWORD=${DB_PASSWORD}
               volumes:
                 - postgres_data:/var/lib/postgresql/data
               command: postgres -c config_file=/etc/postgresql/postgresql.conf
               healthcheck:
                 test: ["CMD-SHELL", "pg_isready -U app -d user_${USER_ID}"]
                 interval: 10s
                 timeout: 5s
                 retries: 5

           volumes:
             postgres_data:
           ```
         - Automate the provisioning of single-user containers
         - Maintain backup and restore procedures per user

   c. Monitoring and Observability:
      - System health monitoring:
        - Focus on CPU, memory, and disk usage for each individual container
        - Track container-specific metrics:
          ```python
          # monitoring.py
          async def collect_container_metrics():
              return {
                  'cpu_usage': await get_container_cpu_usage(),
                  'memory_usage': await get_container_memory_usage(),
                  'disk_usage': await get_container_disk_usage(),
                  'db_connections': await get_db_connection_count(),
                  'message_queue_size': await get_processing_queue_size(),
                  'processing_latency': await get_avg_processing_latency()
              }
          ```
        - Monitor database query performance for the individual instance
      - Error tracking and alerting:
        - Implement error logging within each container
        - Set alert thresholds based on single-user patterns:
          ```python
          # alerts.py
          ALERT_THRESHOLDS = {
              'cpu_usage_percent': 80,
              'memory_usage_percent': 85,
              'disk_usage_percent': 90,
              'message_processing_delay_seconds': 30,
              'failed_requests_per_minute': 5
          }
          ```
      - Performance monitoring:
        - Track real-time metrics for the individual user
        - Measure and optimize message processing latency
        - Monitor database query execution times:
          ```sql
          -- Enable query performance monitoring
          CREATE EXTENSION pg_stat_statements;
          
          -- Query to analyze slow queries
          SELECT query, calls, total_exec_time / calls as avg_exec_time
          FROM pg_stat_statements
          WHERE calls > 10
          ORDER BY avg_exec_time DESC
          LIMIT 10;
          ```

4. Testing & Deployment:
   a. Testing:
      - Unit tests for API endpoints
      - Integration tests for message processing
      - Frontend component tests using Jest and React Testing Library
      - End-to-end testing with Cypress
      - Load testing for concurrent user scenarios

   b. Deployment:
      Frontend (Vercel):
      ```yaml
      # vercel.json
      {
        "env": {
          "NEXT_PUBLIC_API_URL": "https://api.yourdomain.com",
          "NEXT_PUBLIC_WS_URL": "wss://api.yourdomain.com"
        },
        "build": {
          "env": {
            "NEXT_PUBLIC_API_URL": "https://api.yourdomain.com",
            "NEXT_PUBLIC_WS_URL": "wss://api.yourdomain.com"
          }
        }
      }
      ```

      Backend (Docker Compose):
      ```yaml
      version: '3.8'
      services:
        app:
          build: .
          environment:
            - POSTGRES_DB=${DB_NAME}
            - POSTGRES_USER=${DB_USER}
            - POSTGRES_PASSWORD=${DB_PASSWORD}
            - TELEGRAM_API_ID=${TELEGRAM_API_ID}
            - TELEGRAM_API_HASH=${TELEGRAM_API_HASH}
            - CORS_ORIGINS=${CORS_ORIGINS}
            - JWT_SECRET=${JWT_SECRET}
            - REDIS_URL=${REDIS_URL}
          volumes:
            - ./app:/app
          depends_on:
            - db
          ports:
            - "8000:8000"
          healthcheck:
            test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
            interval: 30s
            timeout: 10s
            retries: 3

        db:
          image: postgres:15
          environment:
            - POSTGRES_DB=${DB_NAME}
            - POSTGRES_USER=${DB_USER}
            - POSTGRES_PASSWORD=${DB_PASSWORD}
          volumes:
            - postgres_data:/var/lib/postgresql/data
          healthcheck:
            test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
            interval: 10s
            timeout: 5s
            retries: 5

      volumes:
        postgres_data:
      ```

   c. Monitoring and Observability:
      - System health monitoring:
        - CPU, memory, and disk usage
        - API endpoint response times
        - Database query performance
        - WebSocket connection status
      - Error tracking and alerting:
        - Centralized error logging
        - Alert thresholds for critical metrics
        - Automated incident response
      - Performance monitoring:
        - Real-time user metrics
        - Message processing latency
        - Database query analysis

   d. Backup and Recovery:
      - Automated daily database backups
      - Point-in-time recovery capability
      - Backup verification and testing
      - Disaster recovery procedures

## Development Workflow

1. Local Development:
   - Prerequisites:
     - Docker and Docker Compose
     - Node.js 18+ and npm/yarn
     - Python 3.9+
   - Setup steps:
     ```bash
     # Clone repository
     git clone <repository-url>
     cd <repository>

     # Frontend setup
     cd frontend
     npm install
     npm run dev

     # Backend setup
     cd ../backend
     python -m venv .venv
     source .venv/bin/activate
     pip install -r requirements.txt
     
     # Start services
     docker-compose up -d
     ```

2. Git Workflow:
   - Branch naming convention:
     - feature/* for new features
     - fix/* for bug fixes
     - chore/* for maintenance
   - Pull request requirements:
     - Code review by at least one team member
     - All tests passing
     - No security vulnerabilities
     - Documentation updated

3. CI/CD Pipeline:
   - On pull request:
     - Run linting and tests
     - Build Docker images
     - Deploy to staging environment
   - On merge to main:
     - Deploy to production
     - Run smoke tests
     - Monitor for errors

## Current Status / Progress Tracking

Status: MVP Implementation In Progress

Completed:
1. Backend (FastAPI):
   - Project structure set up
   - Basic FastAPI application created
   - Telethon QR login implementation started
   - Authentication endpoints created
   - CORS configuration added

2. Frontend (Next.js):
   - Project created with Next.js 14, TypeScript, and Tailwind CSS
   - Login page implemented with QR code display
   - Authentication status polling implemented
   - Environment configuration set up

Next Steps:
1. Complete Telethon QR login implementation
2. Test the authentication flow
3. Add message retrieval functionality
4. Create the message display interface

## MVP Development Focus

1. Core Features for MVP:
   - Implement Telethon's direct client QR code-based Telegram login in FastAPI
   - Set up message retrieval from Telegram using Telethon
   - Process messages using the Llama model
   - Display messages and responses on a basic Next.js frontend

2. Next Steps and Action Items for MVP:
   - Set up FastAPI backend:
     - Initialize FastAPI application
     - Implement Telegram authentication via QR code
     - Create an endpoint for message processing
     - Ensure CORS is configured for Next.js frontend

   - Set up Next.js frontend:
     - Create a simple page to display QR code for login
     - Show incoming messages and generated responses
     - Configure environment variables for API communication

   - Deployment and Testing:
     - Deploy backend and frontend using Docker and Vercel
     - Test the end-to-end user journey from login to message display
     - Verify the integration between components

## Incremental Development Plan

### Sprint 1: Enhance Authentication and User Management
- Implement comprehensive access control policies
- Add user registration and profile management
- Improve session management with enhanced JWT token handling
- Conduct security audits and vulnerability assessments

### Sprint 2: Expand Message Processing Capabilities
- Integrate advanced message processing features within the Llama model
- Implement language support for additional languages
- Optimize message processing latency and resource management
- Introduce user-specific message processing preferences

### Sprint 3: Improve UI and User Experience
- Develop a rich message dashboard with advanced filtering and searching capabilities
- Enhance chat interface with multimedia support (images, videos, etc.)
- Implement real-time collaboration features for group chats
- Conduct user experience testing and feedback integration

### Sprint 4: Scale System Performance and Robustness
- Scale message processing for concurrent users
- Enhance database performance with optimized queries and indexing
- Conduct load testing and performance monitoring
- Implement backup and recovery strategies for data integrity

## Executor's Feedback or Assistance Requests

### Previous Status
Ready to test the authentication flow. Need to:
1. Set up Telethon client credentials (API ID and API Hash)
2. Test the Telethon QR code generation and validation
3. Ensure frontend-backend communication supports Telethon authentication

### Current Status - Docker Setup Completed
I've implemented a complete Docker setup for the project that includes:

1. **Docker Compose Configuration**:
   - Created `docker-compose.yml` that sets up and connects:
     - PostgreSQL database
     - FastAPI backend
     - Next.js frontend
   - Implemented health checks for the PostgreSQL database
   - Configured proper dependency ordering between services

2. **Backend Containerization**:
   - Created `backend/Dockerfile` with Python 3.9 base
   - Implemented database initialization script (`app/db/init_db.py`) that:
     - Checks PostgreSQL readiness with retry logic
     - Creates database schema if it doesn't exist
     - Sets up proper indexes for performance

3. **Frontend Containerization**:
   - Created `frontend/Dockerfile` with Node.js 20 base
   - Configured build and runtime environment

4. **Environment Configuration**:
   - Created `.env.docker` template for environment variables
   - Included placeholders for Telegram API credentials

5. **Documentation**:
   - Created `DOCKER_README.md` with:
     - Setup instructions
     - Container management commands
     - Troubleshooting guidance

Next steps:
1. Test the Docker setup with actual Telegram API credentials
2. Verify database initialization and schema creation
3. Ensure proper communication between frontend and backend containers
4. Implement CI/CD pipeline for automated Docker builds and deployments

### Current Status - User Dialog Selection Feature Added

I've added a new database table called `user_selected_dialogs` to give users control over which Telegram conversations the service will process. This enhancement:

1. **Provides User Control**: Users can explicitly select which dialogs (chats) they want the system to process
2. **Supports Prioritization**: Allows setting different priority levels for different conversations
3. **Enables Granular Settings**: Users can enable/disable processing and auto-replies on a per-dialog basis
4. **Stores Processing Preferences**: The JSONB `processing_settings` field can store dialog-specific processing parameters

The table structure includes:
- `selection_id`: Primary key for the selection
- `user_id`: The Telegram user ID who owns this selection
- `dialog_id`: The Telegram dialog/chat ID being selected
- `dialog_name`: Name of the dialog for display purposes
- `is_active`: Whether this selection is currently active
- `processing_enabled`: Whether message processing is enabled for this dialog
- `auto_reply_enabled`: Whether auto-replies are enabled for this dialog
- `response_approval_required`: Whether response approval is required for this dialog
- `priority`: Processing priority (higher numbers = higher priority)
- `created_at` and `updated_at`: Timestamps for creation and updates
- `last_processed_at`: When the dialog was last processed
- `processing_settings`: JSON field for dialog-specific processing parameters

An index has been added to optimize queries by user, active status, and priority.

Next steps for this feature:
1. Create API endpoints for managing selected dialogs
2. Update the message processing logic to filter by selected dialogs
3. Add UI components for dialog selection and configuration
4. Implement dialog-specific processing settings