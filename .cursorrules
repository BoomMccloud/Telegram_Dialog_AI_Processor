# Instructions

You are a multi-agent system coordinator, playing two roles in this environment: Planner and Executor. You will decide the next steps based on the current state of `Multi-Agent Scratchpad` section in the `.cursorrules` file. Your goal is to complete the user's (or business's) final requirements. The specific instructions are as follows:

## Role Descriptions

1. Planner

    * Responsibilities: Perform high-level analysis, break down tasks, define success criteria, evaluate current progress. When doing planning, always use high-intelligence models (OpenAI o1 via `tools/plan_exec_llm.py`). Don't rely on your own capabilities to do the planning.
    * Actions: Invoke the Planner by calling `.venv/bin/python tools/plan_exec_llm.py --prompt {any prompt} --file .cursorrules`. The `--file` option with `.cursorrules` is mandatory to ensure the planner has full context of the current state and previous decisions. You can include additional files in the analysis by adding more `--file` options: `.venv/bin/python tools/plan_exec_llm.py --prompt {any prompt} --file .cursorrules --file {path/to/additional/file}`. It will print out a plan on how to revise the `.cursorrules` file. You then need to actually do the changes to the file. And then reread the file to see what's the next step.

2) Executor

    * Responsibilities: Execute specific tasks instructed by the Planner, such as writing code, running tests, handling implementation details, etc.. The key is you need to report progress or raise questions to the Planner at the right time, e.g. after completion some milestone or after you've hit a blocker.
    * Actions: When you complete a subtask or need assistance/more information, also make incremental writes or modifications to the `Multi-Agent Scratchpad` section in the `.cursorrules` file; update the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections. And then change to the Planner role.

## Document Conventions

* The `Multi-Agent Scratchpad` section in the `.cursorrules` file is divided into several sections as per the above structure. Please do not arbitrarily change the titles to avoid affecting subsequent reading.
* Sections like "Background and Motivation" and "Key Challenges and Analysis" are generally established by the Planner initially and gradually appended during task progress.
* "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" are mainly filled by the Executor, with the Planner reviewing and supplementing as needed.
* "Next Steps and Action Items" mainly contains specific execution steps written by the Planner for the Executor.

## Workflow Guidelines

* After you receive an initial prompt for a new task, update the "Background and Motivation" section, and then invoke the Planner to do the planning.
* When thinking as a Planner, always use the local command line `python tools/plan_exec_llm.py --prompt {any prompt}` to call the o1 model for deep analysis, recording results in sections like "Key Challenges and Analysis" or "High-level Task Breakdown". Also update the "Background and Motivation" section.
* When you as an Executor receive new instructions, use the existing cursor tools and workflow to execute those tasks. After completion, write back to the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections in the `Multi-Agent Scratchpad`.
* If unclear whether Planner or Executor is speaking, declare your current role in the output prompt.
* Continue the cycle unless the Planner explicitly indicates the entire project is complete or stopped. Communication between Planner and Executor is conducted through writing to or modifying the `Multi-Agent Scratchpad` section.

Please note:

* Note the task completion should only be announced by the Planner, not the Executor. If the Executor thinks the task is done, it should ask the Planner for confirmation. Then the Planner needs to do some cross-checking.
* Avoid rewriting the entire document unless necessary;
* Avoid deleting records left by other roles; you can append new paragraphs or mark old paragraphs as outdated;
* When new external information is needed, you can use command line tools (like search_engine.py, llm_api.py), but document the purpose and results of such requests;
* Before executing any large-scale changes or critical functionality, the Executor should first notify the Planner in "Executor's Feedback or Assistance Requests" to ensure everyone understands the consequences.
* During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

# Tools

Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

## Screenshot Verification
The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

1. Screenshot Capture:
```bash
.venv/bin/python tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

2. LLM Verification with Images:
```bash
.venv/bin/python tools/llm_api.py --prompt "Your verification question" --provider {openai|anthropic} --image path/to/screenshot.png
```

Example workflow:
```python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot
screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM
response = query_llm(
    "What is the background color and title of this webpage?",
    provider="openai",  # or "anthropic"
    image_path=screenshot_path
)
print(response)
```

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
.venv/bin/python ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
```

The LLM API supports multiple providers:
- OpenAI (default, model: gpt-4o)
- Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
- DeepSeek (model: deepseek-chat)
- Anthropic (model: claude-3-sonnet-20240229)
- Gemini (model: gemini-pro)
- Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```
.venv/bin/python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```
.venv/bin/python ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- You have a uv python venv in ./.venv. Always use it when running python scripts. It's a uv venv, so use `uv pip install` to install packages. And you need to activate it first. When you see errors like `no such file or directory: .venv/bin/uv`, that means you didn't activate the venv.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.

## Cursor learned

- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use 'seaborn-v0_8' instead of 'seaborn' as the style name due to recent seaborn version changes
- Use `gpt-4o` as the model name for OpenAI. It is the latest GPT model and has vision capabilities as well. `o1` is the most advanced and expensive model from OpenAI. Use it when you need to do reasoning, planning, or get blocked.
- Use `claude-3-5-sonnet-20241022` as the model name for Claude. It is the latest Claude model and has vision capabilities as well.
- When running Python scripts that import from other local modules, use `PYTHONPATH=.` to ensure Python can find the modules. For example: `PYTHONPATH=. python tools/plan_exec_llm.py` instead of just `python tools/plan_exec_llm.py`. This is especially important when using relative imports.
- When using Docker Compose in a project with virtual environments, make sure to activate the virtual environment before running Docker Compose commands to ensure that environment variables from `.env` files are properly loaded and passed to the containers. For example, use `source .venv/bin/activate && docker-compose up -d` instead of just `docker-compose up -d`.
- When mocking an API that uses in-memory session storage (like `client_sessions` dictionary), ensure all dictionaries that store session information are properly synchronized. When creating mock sessions, always populate all required dictionaries with consistent data. For authentication-dependent endpoints, add double-checks to ensure sessions exist in all necessary dictionaries before attempting operations.
- For hybrid development environments, run just the database in Docker while keeping the backend and frontend running locally. This approach gives you the benefits of containerized persistence while maintaining fast development cycles. Use `docker-compose up -d postgres` to start only the database service, then run the backend with environment variables set to connect to the containerized database.
- When switching between development and production modes in a web application, ensure data formats remain consistent. Mock data APIs often have slightly different response formats than real APIs. Always check the response structure in both environments and adjust the backend to maintain format consistency.

# Multi-Agent Scratchpad

## Background and Motivation

This is a Telegram dialog processing system consisting of two main components:

1. `tg_data_downloader.py`: A Telegram data collection service that:
   - Downloads messages from both group and private chats
   - Focuses on unread messages and messages from the last 24 hours
   - Stores the last 20 messages for each relevant dialog
   - Outputs data to telegram_data.json

2. `reply_only_llama3.2.py`: A dialog processing system that:
   - Uses the Llama model for generating responses
   - Implements a sophisticated message processing pipeline
   - Has built-in safety and validation mechanisms
   - Generates context-aware responses

The system appears to be designed for automated processing and responding to Telegram messages using LLM technology.

## Key Challenges and Analysis

1. Technical Architecture Challenges:
   - Integration of FastAPI backend with existing Python scripts
   - Implementation of Telethon's direct client authentication with QR login
   - Management of concurrent user sessions and message processing
   - Handling of periodic background tasks for message downloading
   - Database design for storing user sessions and message history

2. Security Considerations:
   - Secure storage of Telegram API credentials
   - Implement comprehensive access control policies
   - Regular security audits and vulnerability assessments
   - Implement HTTPS everywhere with certificate automation

3. Performance Risks:
   - Scalability of message processing with multiple users
   - Resource management for LLM model instances
   - Network latency in message processing pipeline
   - Database performance with large message volumes

## Verifiable Success Criteria

1. Authentication & Security:
   - Successful QR code-based Telegram login
   - Secure session management
   - Protected API endpoints
   - Encrypted data storage

2. Functionality:
   - Real-time message synchronization
   - Accurate message processing with LLama model
   - Two-way communication working in both web UI and Telegram
   - Background task execution for periodic updates

3. Performance:
   - Message processing latency under 5 seconds
   - UI responsiveness under 200ms
   - Successful handling of concurrent users
   - Efficient database queries

## High-level Task Breakdown

1. Backend Development (FastAPI):
   - Project Setup: Create modular structure, configure environment variables
   - Database Implementation: Use PostgreSQL with proper models/schema
   - Authentication System: Implement Telethon QR login
   - Core API Endpoints: Messages, processing, preferences
   - Background Tasks: Message fetching, processing queue

2. Frontend Development (Next.js):
   - UI Components: Login, dashboard, chat interface, settings
   - State Management: Session handling, real-time updates
   - API Integration: Communication with backend

3. Integration Layer:
   - Connect Next.js and FastAPI
   - Implement database interactions
   - Set up monitoring and observability

4. Testing & Deployment:
   - Unit and integration tests
   - Docker-based deployment
   - Monitoring and backup systems

## Development Workflow

1. Local Development:
   - Prerequisites: Docker, Node.js, Python
   - Setup steps for development environment

2. Git Workflow:
   - Branch naming conventions and PR requirements

3. CI/CD Pipeline:
   - Testing and deployment automation

## Current Status / Progress Tracking

Status: MVP Implementation In Progress

Completed:
1. Backend (FastAPI):
   - Project structure set up
   - Basic FastAPI application created
   - Telethon QR login implementation complete and working
   - Authentication endpoints created and tested
   - CORS configuration added
   - Message retrieval functionality implemented
   - Dialog listing functionality implemented
   - API endpoints for selecting and unselecting dialogs implemented and tested
   - Database storage for user selected dialogs operational

2. Frontend (Next.js):
   - Project created with Next.js 14, TypeScript, and Tailwind CSS
   - Login page implemented with QR code display
   - Authentication status polling implemented 
   - Environment configuration set up
   - Messages page implemented showing chat dialogs
   - Chat selection and filtering UI implemented
   - Dialog selection UI with visual indicators for processing status
   - Start/Stop processing functionality working correctly with API integration
   - Success/error message handling implemented for processing operations

3. Docker Setup:
   - Docker Compose configuration complete with:
     - PostgreSQL database
     - FastAPI backend
     - Next.js frontend
   - Proper service dependencies and health checks configured
   - Environment variable configuration implemented

4. Development Setup:
   - Working hybrid development environment with:
     - Database running in Docker
     - Frontend and backend running locally for development
   - Authentication flow tested and working
   - Chat retrieval working

Next Steps:
1. Implement message processing using the Llama model
2. Add auto-reply functionality
3. Add message history and context view
4. Enhance the UI with more responsive design
5. Implement user preference settings for dialog processing

## MVP Development Focus

1. Core Features for MVP:
   - Implement Telethon's direct client QR code-based Telegram login in FastAPI
   - Set up message retrieval from Telegram using Telethon
   - Store user input and preferences in the database
   - Process messages using the Llama model
   - Display messages and responses on a basic Next.js frontend

2. Next Steps and Action Items for MVP:
   - Implement database storage for user frontend input:
     - Create database models for storing user preferences and selections 
     - Implement API endpoints for saving user input to the database
     - Add database access services for retrieving and updating user data
     - Ensure proper validation and error handling for user input

   - Implement message processing service using the Llama model:
     - Adapt the existing `reply_only_llama3.2.py` script for API use
     - Create endpoints for message processing and response generation
     - Add background tasks for automatic processing
     - Implement storage for processed messages and responses

   - Enhance the frontend for message display:
     - Create components for viewing processed messages
     - Add UI for message history and context
     - Implement response approval workflow
     - Add settings for processing preferences

## Executor's Feedback or Assistance Requests

### Current Status - Docker Setup Completed
I've implemented a complete Docker setup for the project that includes:

1. **Docker Compose Configuration**:
   - Created `docker-compose.yml` that sets up and connects:
     - PostgreSQL database
     - FastAPI backend
     - Next.js frontend
   - Implemented health checks for the PostgreSQL database
   - Configured proper dependency ordering between services

2. **Backend Containerization**:
   - Created `backend/Dockerfile` with Python 3.9 base
   - Implemented database initialization script (`app/db/init_db.py`) that:
     - Checks PostgreSQL readiness with retry logic
     - Creates database schema if it doesn't exist
     - Sets up proper indexes for performance

3. **Frontend Containerization**:
   - Created `frontend/Dockerfile` with Node.js 20 base
   - Configured build and runtime environment

4. **Environment Configuration**:
   - Created `.env.docker` template for environment variables
   - Included placeholders for Telegram API credentials

5. **Documentation**:
   - Created `DOCKER_README.md` with:
     - Setup instructions
     - Container management commands
     - Troubleshooting guidance

Next steps:
1. Test the Docker setup with actual Telegram API credentials
2. Verify database initialization and schema creation
3. Ensure proper communication between frontend and backend containers
4. Implement CI/CD pipeline for automated Docker builds and deployments

### Current Status - User Dialog Selection Feature Added

I've added a new database table called `user_selected_dialogs` to give users control over which Telegram conversations the service will process. This enhancement:

1. **Provides User Control**: Users can explicitly select which dialogs (chats) they want the system to process
2. **Supports Prioritization**: Allows setting different priority levels for different conversations
3. **Enables Granular Settings**: Users can enable/disable processing and auto-replies on a per-dialog basis
4. **Stores Processing Preferences**: The JSONB `processing_settings` field can store dialog-specific processing parameters

The table structure includes key fields for dialog selection, processing settings, and user preferences.

Next steps for this feature:
1. Create API endpoints for managing selected dialogs
2. Update the message processing logic to filter by selected dialogs
3. Add UI components for dialog selection and configuration
4. Implement dialog-specific processing settings

### Current Status - Telegram Login and Chat Retrieval Working

I've successfully implemented and tested:
1. Telegram login via QR code in the frontend and backend
2. Retrieved chat dialogs and messages
3. Set up a working development environment with:
   - Database running in Docker
   - Frontend and backend running locally

### Current Status - User Dialog Selection and Processing Implemented

I've successfully implemented the dialog selection and processing functionality:

1. **Frontend UI Enhancements**:
   - Added visual indicators for dialogs that are being processed (green badge and left border)
   - Implemented chat selection UI with checkboxes and selection controls
   - Added "Start Processing" and "Stop Processing" functionality
   - Improved error/success feedback messages for user actions
   - Implemented tab-based navigation with "Selected" tab showing currently processed dialogs

2. **Backend API Integration**:
   - Integrated with backend APIs for selecting and unselecting dialogs
   - Implemented proper error handling for API requests
   - Fixed issues with API endpoint calls (corrected the DELETE method for unselecting dialogs)
   - Added automatic refresh of selected dialogs after processing changes
   
3. **User Experience Improvements**:
   - Added feedback messages indicating successful actions
   - Implemented visual states for processing operations
   - Added validation to prevent mixed selection of processed/unprocessed dialogs
   - Added loading states during API operations

The system now allows users to:
- Select dialogs from different categories (Direct Messages, Groups, Channels)
- Start processing selected dialogs with visual confirmation
- Stop processing previously selected dialogs
- See which dialogs are currently being processed
- View all currently processed dialogs in the "Selected" tab

Next steps:
1. Implement the actual message processing using the Llama model
2. Add auto-reply functionality based on processed messages
3. Implement more detailed processing settings for individual dialogs
4. Add message history and context viewing capabilities

### Current Status - Next Priority: Database Storage for User Input

Based on our current progress, the next priority is to ensure all user input from the frontend can be properly stored in the database. This includes:

1. **User Dialog Selections**: Store which Telegram conversations the user has selected for processing
2. **User Preferences**: Save user-specific settings for message processing
3. **Processing Settings**: Store dialog-specific processing parameters

The database schema already includes the `user_selected_dialogs` table, but we need to:
1. Create API endpoints for storing user selections
2. Implement frontend components for capturing user preferences
3. Ensure proper data synchronization between frontend and backend
4. Add validation and error handling for user input

### Current Status - Development Environment Improvements

I've implemented several improvements to the development environment:

1. **Hybrid Development Setup**:
   - Created a streamlined workflow where only the PostgreSQL database runs in Docker
   - Configured the backend and frontend to run locally for faster development cycles
   - Set up proper environment variables to connect to the containerized database

2. **Environment Modes**:
   - Added proper production/development mode switching (APP_ENV=production/development)
   - Disabled mock data routes when running in production mode
   - Ensured proper functioning with real Telegram data

3. **User Experience Enhancements**:
   - Added logout functionality to the navigation bar
   - Implemented session management with proper clearing of authentication data
   - Ensured persistence of user selections across login sessions

4. **Data Format Compatibility**:
   - Fixed format inconsistencies between the development and production APIs
   - Added proper API response format matching in the backend
   - Ensured the Dialog interface is consistently formatted between environments
   - Improved error handling and detailed logging for debugging

5. **Authentication Improvements**:
   - Fixed session management issues when switching between environments
   - Ensured proper client connection status checking
   - Added error handling for expired or invalid sessions

The system now provides a smooth development experience with:
- Fast local development for frontend and backend
- Persistence and reliability through containerized database
- Seamless switching between dev/prod environments
- Consistent data formats across all components
- Proper session management with logout capability

Next steps:
1. Complete the integration with the Llama processing model
2. Implement background tasks for automatic message processing
3. Enhance the user interface for message history and context viewing